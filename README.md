# Data_Analysis_Using_Apache_Spark

### This task was given in the 1729 You and AI program.

![super-necessary](https://user-images.githubusercontent.com/73512374/191225382-debca260-c2ec-488b-8e2f-b0b90e6e3f93.png)


### What is Apache Spark??
Apache Spark is a lightning-fast cluster computing technology, designed for fast computation. It is based on Hadoop MapReduce and it extends the MapReduce model to efficiently use it for more types of computations, which includes interactive queries and stream processing. The main feature of Spark is its in-memory cluster computing that increases the processing speed of an application.

### Spark Ecosystem

![image](https://user-images.githubusercontent.com/73512374/187411923-cecaa29c-4d8e-4ca3-a10a-49544fa7f05a.png)

Spark can run on hadoop for storage purpose and can also work standalone. Spark framework has CoreApi's that are core functionalities of spark supports python, Scala, Java, R. with the help of spark we can do real-time-analytics, Machine learning, SQL and Graph processing etc. generally Buisness data is in various formats like transactional data, json, flat files, xml, Mysql, CRM or from databases. Spark work on Parquet file format, Parquet is designed for efficient as well as performant flat columnar storage format of data compared to row based files like CSV or TSV files. 

### Spark architecture
![image](https://user-images.githubusercontent.com/73512374/187414909-073134c6-1c92-43f2-a16c-a3148eaa55fb.png)
